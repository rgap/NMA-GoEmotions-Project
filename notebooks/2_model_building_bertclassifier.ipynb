{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Download and load the train data:\n",
    "train_data_url = 'https://raw.githubusercontent.com/google-research/google-research/2adf640a14f11025ae5a9d0ec493b78530d276d3/goemotions/data/train.tsv'\n",
    "\n",
    "# Load the files into dataframes\n",
    "train_data = pd.read_csv(train_data_url, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>emotion</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>27</td>\n",
       "      <td>ed00q6i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>2</td>\n",
       "      <td>eezlygj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment emotion       id\n",
       "0  Now if he does off himself, everyone will thin...      27  ed00q6i\n",
       "1                     WHY THE FUCK IS BAYLESS ISOING       2  eezlygj"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the data into features and target\n",
    "# comment will be the only feature\n",
    "# emotion will be the target\n",
    "header = [\"comment\", \"emotion\", \"id\"]\n",
    "train_data.columns = header\n",
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making features out of the comment column\n",
    "# we tokenize the comments\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "# we will use the bert-base-uncased tokenizer\n",
    "# this tokenizer will tokenize the comments\n",
    "# and convert them into tokens\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Comments into Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the comments\n",
    "train_data['tokenized_comments'] = train_data['comment'].apply(\n",
    "    lambda x: tokenizer.encode(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>emotion</th>\n",
       "      <th>id</th>\n",
       "      <th>tokenized_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>27</td>\n",
       "      <td>ed00q6i</td>\n",
       "      <td>[101, 2085, 2065, 2002, 2515, 2125, 2370, 1010...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>2</td>\n",
       "      <td>eezlygj</td>\n",
       "      <td>[101, 2339, 1996, 6616, 2003, 3016, 3238, 1116...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>14</td>\n",
       "      <td>ed7ypvh</td>\n",
       "      <td>[101, 2000, 2191, 2014, 2514, 5561, 102]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dirty Southern Wankers</td>\n",
       "      <td>3</td>\n",
       "      <td>ed0bdzj</td>\n",
       "      <td>[101, 6530, 2670, 14071, 11451, 102]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...</td>\n",
       "      <td>26</td>\n",
       "      <td>edvnz26</td>\n",
       "      <td>[101, 18168, 2290, 17931, 3475, 1005, 1056, 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43404</th>\n",
       "      <td>Added you mate well I’ve just got the bow and ...</td>\n",
       "      <td>18</td>\n",
       "      <td>edsb738</td>\n",
       "      <td>[101, 2794, 2017, 6775, 2092, 1045, 1521, 2310...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43405</th>\n",
       "      <td>Always thought that was funny but is it a refe...</td>\n",
       "      <td>6</td>\n",
       "      <td>ee7fdou</td>\n",
       "      <td>[101, 2467, 2245, 2008, 2001, 6057, 2021, 2003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43406</th>\n",
       "      <td>What are you talking about? Anything bad that ...</td>\n",
       "      <td>3</td>\n",
       "      <td>efgbhks</td>\n",
       "      <td>[101, 2054, 2024, 2017, 3331, 2055, 1029, 2505...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43407</th>\n",
       "      <td>More like a baptism, with sexy results!</td>\n",
       "      <td>13</td>\n",
       "      <td>ed1naf8</td>\n",
       "      <td>[101, 2062, 2066, 1037, 18336, 1010, 2007, 791...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43408</th>\n",
       "      <td>Enjoy the ride!</td>\n",
       "      <td>17</td>\n",
       "      <td>eecwmbq</td>\n",
       "      <td>[101, 5959, 1996, 4536, 999, 102]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43409 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment emotion       id  \\\n",
       "0      Now if he does off himself, everyone will thin...      27  ed00q6i   \n",
       "1                         WHY THE FUCK IS BAYLESS ISOING       2  eezlygj   \n",
       "2                            To make her feel threatened      14  ed7ypvh   \n",
       "3                                 Dirty Southern Wankers       3  ed0bdzj   \n",
       "4      OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...      26  edvnz26   \n",
       "...                                                  ...     ...      ...   \n",
       "43404  Added you mate well I’ve just got the bow and ...      18  edsb738   \n",
       "43405  Always thought that was funny but is it a refe...       6  ee7fdou   \n",
       "43406  What are you talking about? Anything bad that ...       3  efgbhks   \n",
       "43407            More like a baptism, with sexy results!      13  ed1naf8   \n",
       "43408                                    Enjoy the ride!      17  eecwmbq   \n",
       "\n",
       "                                      tokenized_comments  \n",
       "0      [101, 2085, 2065, 2002, 2515, 2125, 2370, 1010...  \n",
       "1      [101, 2339, 1996, 6616, 2003, 3016, 3238, 1116...  \n",
       "2               [101, 2000, 2191, 2014, 2514, 5561, 102]  \n",
       "3                   [101, 6530, 2670, 14071, 11451, 102]  \n",
       "4      [101, 18168, 2290, 17931, 3475, 1005, 1056, 22...  \n",
       "...                                                  ...  \n",
       "43404  [101, 2794, 2017, 6775, 2092, 1045, 1521, 2310...  \n",
       "43405  [101, 2467, 2245, 2008, 2001, 6057, 2021, 2003...  \n",
       "43406  [101, 2054, 2024, 2017, 3331, 2055, 1029, 2505...  \n",
       "43407  [101, 2062, 2066, 1037, 18336, 1010, 2007, 791...  \n",
       "43408                  [101, 5959, 1996, 4536, 999, 102]  \n",
       "\n",
       "[43409 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another way to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the tokenizer function\n",
    "\n",
    "#### PADDING means that we add padding to the tokens\n",
    "# for example\n",
    "# if we have the tokens [1, 2, 3, 4, 5]\n",
    "# and we want to pad them to the length of 10\n",
    "# we will add 5 padding tokens to the tokens\n",
    "# so the tokens will look like this: [1, 2, 3, 4, 5, 0, 0, 0, 0, 0]\n",
    "\n",
    "#### TRUNCATION means that we remove tokens from the tokens\n",
    "# for example\n",
    "# if we have the tokens [1, 2, 3, 4, 5]\n",
    "# and we want to truncate them to the length of 3\n",
    "# we will remove the last 2 tokens\n",
    "# so the tokens will look like this: [1, 2, 3]\n",
    "\n",
    "# Why do we need PADDING and TRUNCATION?\n",
    "# The BERT model requires that all the input sequences have the same length\n",
    "# we can achieve this by either padding or truncating the sequences\n",
    "# we can also use a combination of both\n",
    "# for example, using both\n",
    "# we can pad the sequences to a certain length\n",
    "# and if the sequence is longer than the maximum length\n",
    "# we can truncate the sequence to the maximum length\n",
    "\n",
    "#### RETURN_TENSORS means that we want the output to be a PyTorch tensor\n",
    "\n",
    "tokenized_comments = tokenizer(train_data['comment'].to_list(), padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2085, 2065,  ...,    0,    0,    0],\n",
       "        [ 101, 2339, 1996,  ...,    0,    0,    0],\n",
       "        [ 101, 2000, 2191,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2054, 2024,  ...,    0,    0,    0],\n",
       "        [ 101, 2062, 2066,  ...,    0,    0,    0],\n",
       "        [ 101, 5959, 1996,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### TOKENIZED_COMMENTS is a dictionary\n",
    "# it contains the INPUT_IDS, ATTENTION_MASK, and TOKEN_TYPE_IDS\n",
    "# input_ids are the tokenized comments\n",
    "\n",
    "#### ATTENTION_MASK is a tensor that has the same length as the input_ids\n",
    "# it contains 1s where the input_ids are and 0s where the padding tokens are\n",
    "\n",
    "#### TOKEN_TYPE_IDS is a tensor that has the same length as the input_ids\n",
    "# it contains 0s where the first sentence is and 1s where the second sentence is\n",
    "# since we only have one sentence, all the values are 0s\n",
    "\n",
    "# So it separates the text into sentences?\n",
    "# Yes, it separates the text into sentences\n",
    "# but since we only have one sentence, all the values are 0s\n",
    "\n",
    "tokenized_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Building\n",
    "\n",
    "We try BertForSequenceClassification now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d071b682dd594782922d8800c4525ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "# Load the BERT model\n",
    "# we will use the bert-base-uncased model\n",
    "# this model will classify the comments into 28 emotions\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the research question, the research question could be \"Can we predict the sentiment of a text?\"\n",
    "\n",
    "Our first research question is now:\n",
    "\n",
    "**How can different transformer-based models classify textual emotions effectively?**\n",
    "\n",
    "We start with the BERT Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
