{"cells":[{"cell_type":"markdown","metadata":{"id":"hgA4zWpOOA5D"},"source":["## Data Collection and Feature Engineering steps"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"SVcsYdSqPja9","executionInfo":{"status":"ok","timestamp":1722295798526,"user_tz":300,"elapsed":16368,"user":{"displayName":"Nolan Eduardo Casas","userId":"15662852858614813971"}}},"outputs":[],"source":["from IPython.display import clear_output\n","!pip install transformers[torch]\n","!pip install nlpaug\n","clear_output()"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":136613,"status":"ok","timestamp":1722295935135,"user":{"displayName":"Nolan Eduardo Casas","userId":"15662852858614813971"},"user_tz":300},"id":"_0H-B2c-OA5F","outputId":"8e5b2037-3746-4f7a-e78a-6a3dad94dbc5"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","X_train shape: torch.Size([68200, 316])\n","y_train shape: torch.Size([68200])\n","y_train unique: [0 1 2 3 4]\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import nlpaug.augmenter.word as naw\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import nltk\n","from transformers import BertTokenizer\n","# Ensure you have the necessary resources downloaded\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","def load_data(data_url):\n","    return pd.read_csv(data_url, sep='\\t')\n","\n","def preprocess_data(data):\n","    header = [\"comment\", \"emotion\", \"id\"]\n","    data.columns = header\n","    data = data[['comment', 'emotion']]\n","    data = data[data['emotion'].apply(lambda x: len(x.split(',')) == 1)]\n","    data['emotion'] = data['emotion'].apply(lambda x: ''.join(filter(str.isdigit, str(x)))).astype(int)\n","    return data\n","\n","def map_emotions(data):\n","    emotions_dict = {\n","        0: \"admiration\", 1: \"amusement\", 2: \"anger\", 3: \"annoyance\", 4: \"approval\",\n","        5: \"caring\", 6: \"confusion\", 7: \"curiosity\", 8: \"desire\", 9: \"disappointment\",\n","        10: \"disapproval\", 11: \"disgust\", 12: \"embarrassment\", 13: \"excitement\", 14: \"fear\",\n","        15: \"gratitude\", 16: \"grief\", 17: \"joy\", 18: \"love\", 19: \"nervousness\",\n","        20: \"optimism\", 21: \"pride\", 22: \"realization\", 23: \"relief\", 24: \"remorse\",\n","        25: \"sadness\", 26: \"surprise\", 27: \"neutral\"\n","    }\n","    emotion_mapping = {\n","        \"admiration\": \"positive_intent\", \"amusement\": \"positive_intent\", \"anger\": \"negative_intent\", \"annoyance\": \"negative_intent\",\n","        \"approval\": \"positive_intent\", \"caring\": \"positive_intent\", \"confusion\": \"inquiry\", \"curiosity\": \"inquiry\",\n","        \"desire\": \"positive_intent\", \"disappointment\": \"negative_intent\", \"disapproval\": \"negative_intent\", \"disgust\": \"negative_intent\",\n","        \"embarrassment\": \"negative_intent\", \"excitement\": \"positive_intent\", \"fear\": \"urgency\", \"gratitude\": \"positive_intent\",\n","        \"grief\": \"negative_intent\", \"joy\": \"positive_intent\", \"love\": \"positive_intent\", \"nervousness\": \"urgency\",\n","        \"optimism\": \"positive_intent\", \"pride\": \"positive_intent\", \"realization\": \"inquiry\", \"relief\": \"positive_intent\",\n","        \"remorse\": \"negative_intent\", \"sadness\": \"negative_intent\", \"surprise\": \"positive_intent\", \"neutral\": \"neutral\"\n","    }\n","    category_to_int_mapping = {\n","        \"neutral\": 0, \"negative_intent\": 1, \"positive_intent\": 2, \"inquiry\": 3, \"urgency\": 4\n","    }\n","    def map_emotion(emotion_id):\n","        original_emotion = emotions_dict.get(int(emotion_id), 'other')\n","        category = emotion_mapping.get(original_emotion, 'other')\n","        return category_to_int_mapping.get(category, -1)  # Use -1 for any unmapped categories\n","    data['emotion'] = data['emotion'].apply(map_emotion)\n","    return data\n","\n","def sample_data(data, fraction=1.0):\n","    sampled = data.groupby('emotion').sample(frac=fraction, replace=False)\n","    return sampled.reset_index(drop=True)\n","\n","def augment_data(train_data):\n","    # Initialize the augmenter\n","    aug = naw.SynonymAug(aug_src='wordnet')\n","\n","    def augment_comments(comments, num_augments):\n","        augmented_comments = []\n","        for comment in comments:\n","            comment_str = ' '.join(comment) if isinstance(comment, list) else str(comment)\n","            for _ in range(num_augments):\n","                augmented = aug.augment(comment_str)\n","                # Check if augmented is a list and take the first element if so\n","                if isinstance(augmented, list):\n","                    augmented_comments.append(str(augmented[0]))\n","                else:\n","                    augmented_comments.append(str(augmented))\n","        return augmented_comments\n","\n","    def augment_class(data, class_label, target_count):\n","        class_data = data[data['emotion'] == class_label]\n","        current_count = len(class_data)\n","        augment_count = target_count - current_count\n","        if augment_count > 0:\n","            num_augments = augment_count // current_count + 1\n","            augmented_comments = augment_comments(class_data['comment'].tolist(), num_augments)[:augment_count]\n","            augmented_emotions = [class_label] * len(augmented_comments)\n","            return pd.DataFrame({'comment': augmented_comments, 'emotion': augmented_emotions})\n","        return pd.DataFrame()\n","\n","    # Calculate the class distribution\n","    class_counts = train_data['emotion'].value_counts()\n","    max_count = class_counts.max()\n","\n","    # Augment minority classes\n","    augmented_data = []\n","    for class_label, count in class_counts.items():\n","        if count < max_count:\n","            augmented_data.append(augment_class(train_data, class_label, max_count))\n","\n","    if augmented_data:\n","        augmented_data = pd.concat(augmented_data).reset_index(drop=True)\n","    else:\n","        augmented_data = pd.DataFrame(columns=['comment', 'emotion'])\n","\n","    # Combine with the original data\n","    augmented_train_data = pd.concat([train_data, augmented_data]).reset_index(drop=True)\n","\n","    # Ensure all comments in the final dataframe are strings\n","    augmented_train_data['comment'] = augmented_train_data['comment'].apply(str)\n","\n","    return augmented_train_data\n","\n","def remove_stopwords(data):\n","    stop_words = set(stopwords.words('english'))\n","    data['comment'] = data['comment'].apply(lambda comment: ' '.join([word for word in word_tokenize(comment) if word.lower() not in stop_words]))\n","    return data\n","\n","def tokenize_data(comments):\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    return tokenizer(comments, padding=True, truncation=True, return_tensors='pt')\n","\n","def transform_data(data_url, augmentation=True):\n","    data = load_data(data_url)\n","    data = preprocess_data(data)\n","    data = map_emotions(data)\n","    data = sample_data(data)\n","    if augmentation:\n","        data = augment_data(data)\n","    # data = remove_stopwords(data)\n","    tokenized_comments = tokenize_data(data['comment'].to_list())\n","    return tokenized_comments, torch.tensor(data['emotion'].values)\n","\n","# URLs for train and validation data\n","train_data_url = 'https://github.com/google-research/google-research/raw/master/goemotions/data/train.tsv'\n","validation_data_url = 'https://github.com/google-research/google-research/raw/master/goemotions/data/dev.tsv'\n","\n","# Process train and validation data\n","train_tokenized_comments, y_train = transform_data(train_data_url)\n","validation_tokenized_comments, y_validation = transform_data(validation_data_url, augmentation=False)\n","\n","# Prepare data\n","X_train = train_tokenized_comments['input_ids']\n","attention_masks_train = train_tokenized_comments['attention_mask']\n","X_validation = validation_tokenized_comments['input_ids']\n","attention_masks_validation = validation_tokenized_comments['attention_mask']\n","\n","print()\n","print(\"X_train shape:\", X_train.shape)\n","print(\"y_train shape:\", y_train.shape)\n","print(\"y_train unique:\", np.unique(y_train))\n"]},{"cell_type":"markdown","metadata":{"id":"51iwH5DzOA5H"},"source":["# 1. Model Building\n","\n","Here's the current research question:\n","\n","**\"Can we predict the sentiment of a textual comment?\"**"]},{"cell_type":"markdown","metadata":{"id":"OmwvMxinOA5H"},"source":["### Initialization"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1722295935135,"user":{"displayName":"Nolan Eduardo Casas","userId":"15662852858614813971"},"user_tz":300},"id":"HlSyok5zOA5H","outputId":"00fd401b-fdde-45b3-9e44-9a3e019765ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","CUDA Device: Tesla T4\n","CUDA memory allocated: 0.00 GB\n"]}],"source":["def get_device():\n","    if torch.cuda.is_available():\n","        return torch.device(\"cuda\")\n","    if torch.backends.mps.is_available():\n","        return torch.device(\"mps\")\n","    return torch.device(\"cpu\")\n","\n","def print_device_info(device):\n","    print(f\"Using device: {device}\")\n","    if device.type == \"cuda\":\n","        print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n","        print(f\"CUDA memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n","    elif device.type == \"mps\":\n","        torch.mps.empty_cache()\n","        print(f\"MPS memory allocated: {torch.mps.current_allocated_memory() / 1e9:.2f} GB\")\n","    elif device.type == \"cpu\":\n","        print(\"No GPU available. Using CPU.\")\n","\n","device = get_device()\n","print_device_info(device)"]},{"cell_type":"markdown","metadata":{"id":"LwXME0NuOA5H"},"source":["### Define the model"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"9WVVrFlVOA5H","executionInfo":{"status":"ok","timestamp":1722295935135,"user_tz":300,"elapsed":2,"user":{"displayName":"Nolan Eduardo Casas","userId":"15662852858614813971"}}},"outputs":[],"source":["from transformers import BertForSequenceClassification\n","# Load the BERT model\n","# we will use the bert-base-uncased model\n","# this model will classify the comments into 10 emotions\n","\n","# Modify the BERT model to include dropout\n","class BertForSequenceClassificationWithDropout(BertForSequenceClassification):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.dropout = torch.nn.Dropout(p=0.2)\n","\n","    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","        pooled_output = outputs[1]\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","\n","        loss = None\n","        if labels is not None:\n","            loss_fct = torch.nn.CrossEntropyLoss()\n","            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return torch.nn.utils.rnn.PackedSequence(logits=logits, loss=loss, hidden_states=outputs.hidden_states, attentions=outputs.attentions)"]},{"cell_type":"markdown","metadata":{"id":"XNG5tEjbOA5H"},"source":["### Defining the emotion dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":722,"referenced_widgets":["17672e221deb4756baf8e33ca359b9d8","e326226b3d0f4b21b87dfde32c37e6d4","f0e6caed7fa144f89e55ffcc54522b4e","7b9fcb5fb32c4381ba41502cfe3560c7","10ec92b81d5740b0b1249519fc300202","18d1792ad0be42c1af0581ce7e21005b","be17f727d614405e888871933170992c","2cb099bbb0c940c4a41285d475d65047","842b63a65cda478aad04307f215b3413","3a182568f0024208afea74b2bee6602f","3c08aebe53384681a4d7048c1ae49bc6","528595db926641e7be11c1d174618221","3cbd9ebcdb3948349975c4435879c002","77ffda40d7a7469dbdcfe87e8fbb7f16","ae021e9c31ce428793b0e93f76eee973","9e5804ce906042fe98239710c7424699","1a53752a5ffa4db4851c0ac2aa6bfe47","9733588a5dd740d9a7e5b28a75f32e4d","e2e80c443c9c49f4bb561482e2b4e601","406e2e5eba5c48fba7a4df94a57c061f","e86b15e87e63453a8df735f771f42530","56d1110a600d4a1281e7c6b13faae620"]},"id":"FwN2O_FBOA5H","outputId":"9279ae02-cca8-4287-965a-da4de94d1ebc"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17672e221deb4756baf8e33ca359b9d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"528595db926641e7be11c1d174618221"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassificationWithDropout were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='301' max='31950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  301/31950 2:09:17 < 228:05:21, 0.04 it/s, Epoch 0.28/30]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>20</td>\n","      <td>1.714700</td>\n","      <td>1.644410</td>\n","      <td>0.137876</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.635100</td>\n","      <td>1.581078</td>\n","      <td>0.233492</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.574100</td>\n","      <td>1.516528</td>\n","      <td>0.289801</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>1.499900</td>\n","      <td>1.417321</td>\n","      <td>0.448662</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.350100</td>\n","      <td>1.233041</td>\n","      <td>0.505549</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>1.178400</td>\n","      <td>1.101497</td>\n","      <td>0.535111</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>1.064500</td>\n","      <td>0.962895</td>\n","      <td>0.614257</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.951300</td>\n","      <td>0.916247</td>\n","      <td>0.642770</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.957700</td>\n","      <td>0.853908</td>\n","      <td>0.670149</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.835600</td>\n","      <td>0.822837</td>\n","      <td>0.692804</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.836600</td>\n","      <td>0.798540</td>\n","      <td>0.692072</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.764600</td>\n","      <td>0.773206</td>\n","      <td>0.706721</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.825100</td>\n","      <td>0.739574</td>\n","      <td>0.713749</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.791100</td>\n","      <td>0.755165</td>\n","      <td>0.709633</td>\n","    </tr>\n","  </tbody>\n","</table><p>\n","    <div>\n","      \n","      <progress value='2033' max='2132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2033/2132 06:44 < 00:19, 5.03 it/s]\n","    </div>\n","    "]},"metadata":{}}],"source":["from sklearn.model_selection import KFold\n","from torch.utils.data import Dataset\n","from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","\n","# Create a Dataset class\n","class EmotionDataset(Dataset):\n","    def __init__(self, input_ids, attention_masks, labels):\n","        self.input_ids = input_ids\n","        self.attention_masks = attention_masks\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'input_ids': self.input_ids[idx],\n","            'attention_mask': self.attention_masks[idx],\n","            'labels': self.labels[idx]\n","        }\n","\n","# Cross-validation setup\n","kf = KFold(n_splits=2, shuffle=True)\n","\n","# Prepare datasets for cross-validation\n","datasets = []\n","for train_index, val_index in kf.split(X_train):\n","    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n","    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n","    attention_masks_train_fold, attention_masks_val_fold = attention_masks_train[train_index], attention_masks_train[val_index]\n","\n","    train_dataset_fold = EmotionDataset(X_train_fold, attention_masks_train_fold, y_train_fold)\n","    val_dataset_fold = EmotionDataset(X_val_fold, attention_masks_val_fold, y_val_fold)\n","\n","    datasets.append((train_dataset_fold, val_dataset_fold))\n","\n","\n","from sklearn.metrics import f1_score\n","\n","def compute_f1(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return {'f1': f1_score(labels, predictions, average='macro')}\n","\n","\n","\n","# Define training arguments with advanced scheduler\n","training_args = TrainingArguments(\n","    output_dir='./results',  # Directory to save the model checkpoints\n","    num_train_epochs=30,  # Increased number of epochs\n","    per_device_train_batch_size=8,  # Smaller batch size\n","    per_device_eval_batch_size=16,  # Batch size for evaluation\n","    warmup_steps=100,  # Number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,  # Strength of weight decay\n","    logging_dir='./logs',  # Directory for storing logs\n","    logging_steps=20,  # Log every X updates steps\n","    evaluation_strategy=\"steps\",  # Evaluate the model every X steps\n","    save_strategy=\"steps\",  # Save the model checkpoint every X steps\n","    save_steps=20,  # Steps interval for saving model checkpoint\n","    eval_steps=20,  # Steps interval for evaluation\n","    gradient_accumulation_steps=4,  # Number of updates steps to accumulate before performing a backward/update pass\n","    fp16=True,  # Use 16-bit (mixed) precision training\n","    learning_rate=2e-5,  # The initial learning rate for AdamW optimizer\n","    load_best_model_at_end=True,  # Load the best model when finished training\n","    lr_scheduler_type=\"cosine_with_restarts\",  # Cosine annealing scheduler with restarts\n","    save_total_limit=2,  # Limit the total amount of checkpoints. Deletes the older checkpoints.\n","    metric_for_best_model=\"f1\",\n","    greater_is_better=True,  # F1 score should be maximized\n","    max_grad_norm=1.0,  # Added gradient clipping (corrected from gradient_clipping)\n",")\n","\n","# Train model with cross-validation\n","all_train_losses = []\n","all_eval_losses = []\n","all_train_f1_scores = []\n","all_eval_f1_scores = []\n","\n","for train_dataset_fold, val_dataset_fold in datasets:\n","    # Initialize a new model for each fold\n","    model = BertForSequenceClassificationWithDropout.from_pretrained('bert-large-uncased', num_labels=5)\n","    model.to(device)\n","\n","    # Calculate the number of training steps\n","    num_train_steps = len(train_dataset_fold) * training_args.num_train_epochs // training_args.per_device_train_batch_size\n","\n","    # Create optimizer and learning rate scheduler\n","    optimizer = AdamW(model.parameters(), lr=training_args.learning_rate, eps=1e-8)\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_train_steps)\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset_fold,\n","        eval_dataset=val_dataset_fold,\n","        compute_metrics=compute_f1,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=15)],\n","        optimizers=(optimizer, scheduler)\n","    )\n","\n","    train_result = trainer.train()\n","\n","    # Extract loss and F1 score values from the logs\n","    for log in trainer.state.log_history:\n","        if 'loss' in log:\n","            all_train_losses.append(log['loss'])\n","        if 'eval_loss' in log:\n","            all_eval_losses.append(log['eval_loss'])\n","        if 'f1' in log:\n","            all_train_f1_scores.append(log['f1'])\n","        if 'eval_f1' in log:\n","            all_eval_f1_scores.append(log['eval_f1'])\n","\n","    # Evaluate the model on the validation set\n","    eval_result = trainer.evaluate()\n","    print(f\"Evaluation result: {eval_result}\")\n","\n","# Print summary statistics\n","print(f\"Average train loss: {np.mean(all_train_losses):.4f}\")\n","print(f\"Average eval loss: {np.mean(all_eval_losses):.4f}\")\n","print(f\"Average train F1 score: {np.mean(all_train_f1_scores):.4f}\")\n","print(f\"Average eval F1 score: {np.mean(all_eval_f1_scores):.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGwXHQx2OA5I"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Plot the losses\n","plt.plot(all_train_losses, label='Training Loss')\n","plt.plot(all_eval_losses, label='Validation Loss')\n","plt.xlabel('Logging Steps')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss Over Cross-Validation Folds')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"giBiy0Q7KwKW"},"outputs":[],"source":["plt.subplot(1, 2, 2)\n","plt.plot(all_train_f1_scores, label='Train F1')\n","plt.plot(all_eval_f1_scores, label='Eval F1')\n","plt.title('F1 Score Curves')\n","plt.xlabel('Step')\n","plt.ylabel('F1 Score')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"r4f4V5i4ciXt"},"source":["### Save Model (only from google drive)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8iuldGita6IC"},"outputs":[],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGJJ54qDbWEt"},"outputs":[],"source":["import os\n","\n","project_name = \"NMA-GoEmotions-Project\"\n","model_save_path = f'/content/drive/MyDrive/@NMA_Projects/{project_name}/{project_name}/model'\n","\n","source_path = './results/'\n","\n","# Create the destination directory if it doesn't exist\n","os.makedirs(model_save_path, exist_ok=True)\n","\n","# Save only the classification layer weights\n","classifier_weights = model.classifier.state_dict()\n","torch.save(classifier_weights, os.path.join(model_save_path, 'classifier_weights_bert_newmapping_f1.pt'))\n","\n","print(f\"Model copied to Google Drive at: {model_save_path}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"widgets":{"application/vnd.jupyter.widget-state+json":{"17672e221deb4756baf8e33ca359b9d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e326226b3d0f4b21b87dfde32c37e6d4","IPY_MODEL_f0e6caed7fa144f89e55ffcc54522b4e","IPY_MODEL_7b9fcb5fb32c4381ba41502cfe3560c7"],"layout":"IPY_MODEL_10ec92b81d5740b0b1249519fc300202"}},"e326226b3d0f4b21b87dfde32c37e6d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18d1792ad0be42c1af0581ce7e21005b","placeholder":"​","style":"IPY_MODEL_be17f727d614405e888871933170992c","value":"config.json: 100%"}},"f0e6caed7fa144f89e55ffcc54522b4e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2cb099bbb0c940c4a41285d475d65047","max":571,"min":0,"orientation":"horizontal","style":"IPY_MODEL_842b63a65cda478aad04307f215b3413","value":571}},"7b9fcb5fb32c4381ba41502cfe3560c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a182568f0024208afea74b2bee6602f","placeholder":"​","style":"IPY_MODEL_3c08aebe53384681a4d7048c1ae49bc6","value":" 571/571 [00:00&lt;00:00, 26.4kB/s]"}},"10ec92b81d5740b0b1249519fc300202":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18d1792ad0be42c1af0581ce7e21005b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be17f727d614405e888871933170992c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2cb099bbb0c940c4a41285d475d65047":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"842b63a65cda478aad04307f215b3413":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3a182568f0024208afea74b2bee6602f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c08aebe53384681a4d7048c1ae49bc6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"528595db926641e7be11c1d174618221":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3cbd9ebcdb3948349975c4435879c002","IPY_MODEL_77ffda40d7a7469dbdcfe87e8fbb7f16","IPY_MODEL_ae021e9c31ce428793b0e93f76eee973"],"layout":"IPY_MODEL_9e5804ce906042fe98239710c7424699"}},"3cbd9ebcdb3948349975c4435879c002":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a53752a5ffa4db4851c0ac2aa6bfe47","placeholder":"​","style":"IPY_MODEL_9733588a5dd740d9a7e5b28a75f32e4d","value":"model.safetensors: 100%"}},"77ffda40d7a7469dbdcfe87e8fbb7f16":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2e80c443c9c49f4bb561482e2b4e601","max":1344951957,"min":0,"orientation":"horizontal","style":"IPY_MODEL_406e2e5eba5c48fba7a4df94a57c061f","value":1344951957}},"ae021e9c31ce428793b0e93f76eee973":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e86b15e87e63453a8df735f771f42530","placeholder":"​","style":"IPY_MODEL_56d1110a600d4a1281e7c6b13faae620","value":" 1.34G/1.34G [00:15&lt;00:00, 126MB/s]"}},"9e5804ce906042fe98239710c7424699":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a53752a5ffa4db4851c0ac2aa6bfe47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9733588a5dd740d9a7e5b28a75f32e4d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2e80c443c9c49f4bb561482e2b4e601":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"406e2e5eba5c48fba7a4df94a57c061f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e86b15e87e63453a8df735f771f42530":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56d1110a600d4a1281e7c6b13faae620":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}