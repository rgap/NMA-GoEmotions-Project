{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Feature Engineering steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xd/gw_pmm5d23s8dm3h09tn28x00000gn/T/ipykernel_74025/2374661342.py:30: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_data = filtered_data.groupby('emotion').apply(lambda x: x.sample(frac=0.1)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 2 most common emotions: ['neutral', 'admiration']\n",
      "Sampled data shape: (1553, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Download and load the train data\n",
    "train_data_url = 'https://raw.githubusercontent.com/google-research/google-research/master/goemotions/data/train.tsv'\n",
    "train_data = pd.read_csv(train_data_url, sep='\\t')\n",
    "\n",
    "# Comment will be the only feature, emotion will be the target (multiple labels)\n",
    "header = [\"comment\", \"emotion\", \"id\"]\n",
    "train_data.columns = header\n",
    "\n",
    "# Remove instances with more than one emotion from each dataset\n",
    "train_data = train_data[train_data['emotion'].apply(lambda x: len(x.split(',')) == 1)]\n",
    "\n",
    "# Convert emotion column into integers\n",
    "train_data['emotion'] = train_data['emotion'].apply(lambda x: ''.join(filter(str.isdigit, str(x)))).astype(int)\n",
    "\n",
    "# Determine the frequency of each emotion\n",
    "emotion_counts = train_data['emotion'].value_counts()\n",
    "\n",
    "# Select the top most common emotions\n",
    "top_emotions = emotion_counts.head(2).index\n",
    "\n",
    "# Filter the dataset to include only instances with the top 2 emotions\n",
    "filtered_data = train_data[train_data['emotion'].isin(top_emotions)]\n",
    "\n",
    "# Take only 10% of the data for each of the top 2 emotions\n",
    "sampled_data = filtered_data.groupby('emotion').apply(lambda x: x.sample(frac=0.1)).reset_index(drop=True)\n",
    "\n",
    "# Exclude the grouping columns after the groupby operation\n",
    "sampled_data = sampled_data.reset_index(drop=True)\n",
    "\n",
    "# Tokenize the comments using the BERT tokenizer (Convert comments into Tokens)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_comments = tokenizer(sampled_data['comment'].to_list(), padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Bert will use these features and labels\n",
    "X_train = tokenized_comments['input_ids']\n",
    "attention_masks = tokenized_comments['attention_mask']\n",
    "y_train = torch.tensor(sampled_data['emotion'].values)\n",
    "\n",
    "# Map the top emotions to their labels\n",
    "emotions_dict = {\n",
    "    0: \"admiration\", 1: \"amusement\", 2: \"anger\", 3: \"annoyance\", 4: \"approval\",\n",
    "    5: \"caring\", 6: \"confusion\", 7: \"curiosity\", 8: \"desire\", 9: \"disappointment\",\n",
    "    10: \"disapproval\", 11: \"disgust\", 12: \"embarrassment\", 13: \"excitement\", 14: \"fear\",\n",
    "    15: \"gratitude\", 16: \"grief\", 17: \"joy\", 18: \"love\", 19: \"nervousness\",\n",
    "    20: \"optimism\", 21: \"pride\", 22: \"realization\", 23: \"relief\", 24: \"remorse\",\n",
    "    25: \"sadness\", 26: \"surprise\", 27: \"neutral\"\n",
    "}\n",
    "top_emotions_dict = {k: emotions_dict[k] for k in top_emotions}\n",
    "\n",
    "print()\n",
    "print(\"Top 2 most common emotions:\", [top_emotions_dict[e] for e in top_emotions])\n",
    "print(\"Sampled data shape:\", sampled_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model Building - ML Classifiers\n",
    "\n",
    "We try classic ML Classifiers first.\n",
    "\n",
    "Here's the current research question:\n",
    "\n",
    "**\"Can we predict the sentiment of a textual comment?\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I wont use  -->  from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',          # Output directory\n",
    "#     num_train_epochs=3,              # Number of training epochs\n",
    "#     per_device_train_batch_size=8,   # Batch size for training\n",
    "#     per_device_eval_batch_size=8,    # Batch size for evaluation\n",
    "#     warmup_steps=500,                # Number of warmup steps\n",
    "#     weight_decay=0.01,               # Weight decay\n",
    "#     logging_dir='./logs',            # Directory for storing logs\n",
    "#     logging_steps=10,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,                         # The instantiated ðŸ¤— Transformers model to be trained\n",
    "#     args=training_args,                  # Training arguments, defined above\n",
    "#     train_dataset=train_dataset,         # Training dataset\n",
    "#     eval_dataset=eval_dataset            # Evaluation dataset\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "# Load the BERT model\n",
    "# we will use the bert-base-uncased model\n",
    "# this model will classify the comments into 10 emotions\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.to(device);  # Move the model to the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the optimizer and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__output_dir='./results'\n",
    "__num_train_epochs=1\n",
    "__per_device_train_batch_size=2\n",
    "__warmup_steps=50\n",
    "__weight_decay=0.1\n",
    "__logging_dir='./logs'\n",
    "__logging_steps=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Create a custom dataset class\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, inputs, masks, labels):\n",
    "        self.inputs = inputs\n",
    "        self.masks = masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs[idx],\n",
    "            'attention_mask': self.masks[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = EmotionDataset(X_train, attention_masks, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=__per_device_train_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current allocated memory: 0.41 GB\n"
     ]
    }
   ],
   "source": [
    "torch.mps.empty_cache()\n",
    "print(\"Current allocated memory: {:.2f} GB\".format(torch.mps.current_allocated_memory() / (1024 * 1024 * 1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0, Loss: 0.0\n",
      "Epoch 1, Step 50, Loss: 0.0\n",
      "Epoch 1, Step 100, Loss: 0.035982392728328705\n",
      "Epoch 1, Step 150, Loss: 0.0\n",
      "Epoch 1, Step 200, Loss: 0.0\n",
      "Epoch 1, Step 250, Loss: 0.0\n",
      "Epoch 1, Step 300, Loss: 0.0\n",
      "Epoch 1, Step 350, Loss: 0.004793396219611168\n",
      "Epoch 1, Step 400, Loss: 0.0\n",
      "Epoch 1, Step 450, Loss: 0.0035409717820584774\n",
      "Epoch 1, Step 500, Loss: 0.0\n",
      "Epoch 1, Step 550, Loss: 0.0\n",
      "Epoch 1, Step 600, Loss: 0.0014696555444970727\n",
      "Epoch 1, Step 650, Loss: 0.0\n",
      "Epoch 1, Step 700, Loss: 0.0016236735973507166\n",
      "Epoch 1, Step 750, Loss: 0.0\n",
      "Epoch 1 completed. Average Loss: 0.02225907958287055\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Step 1: Define optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=__weight_decay)\n",
    "total_steps = len(train_loader) * __num_train_epochs \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=__warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# Directory for saving model and logs\n",
    "output_dir = __output_dir\n",
    "logging_dir = __logging_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(logging_dir, exist_ok=True)\n",
    "\n",
    "# Step 2: Training loop\n",
    "num_train_epochs = __num_train_epochs\n",
    "logging_steps = __logging_steps\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_train_epochs):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # Move batch data to the GPU\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        masks = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs, attention_mask=masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        if step % logging_steps == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Step {step}, Loss: {loss.item()}\")\n",
    "    \n",
    "    epoch_loss /= len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1} completed. Average Loss: {epoch_loss}\")\n",
    "    \n",
    "    # Save the model after each epoch\n",
    "    model.save_pretrained(os.path.join(output_dir, f\"model_epoch_{epoch + 1}\"))\n",
    "    tokenizer.save_pretrained(os.path.join(output_dir, f\"model_epoch_{epoch + 1}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
