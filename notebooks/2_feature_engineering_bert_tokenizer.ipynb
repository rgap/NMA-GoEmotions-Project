{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Download and load the train data:\n",
    "train_data_url = 'https://raw.githubusercontent.com/google-research/google-research/2adf640a14f11025ae5a9d0ec493b78530d276d3/goemotions/data/train.tsv'\n",
    "\n",
    "# Load the files into dataframes\n",
    "train_data = pd.read_csv(train_data_url, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>emotion</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>27</td>\n",
       "      <td>ed00q6i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>2</td>\n",
       "      <td>eezlygj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment emotion       id\n",
       "0  Now if he does off himself, everyone will thin...      27  ed00q6i\n",
       "1                     WHY THE FUCK IS BAYLESS ISOING       2  eezlygj"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comment will be the only feature\n",
    "# emotion will be the target (multiple labels)\n",
    "header = [\"comment\", \"emotion\", \"id\"]\n",
    "train_data.columns = header\n",
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing instances with more than one emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove instances with more than one emotion from each dataset\n",
    "train_data = train_data[train_data['emotion'].apply(lambda x: len(x.split(',')) == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert emotion column into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert emotion column into integers\n",
    "train_data['emotion'] = train_data['emotion'].apply(lambda x: ''.join(filter(str.isdigit, str(x)))).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the BERT tokenizer to process the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making features out of the comment column\n",
    "# we tokenize the comments\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "# we will use the bert-base-uncased tokenizer\n",
    "# this tokenizer will tokenize the comments\n",
    "# and convert them into tokens\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Comments into Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>emotion</th>\n",
       "      <th>id</th>\n",
       "      <th>tokenized_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>27</td>\n",
       "      <td>ed00q6i</td>\n",
       "      <td>[101, 2085, 2065, 2002, 2515, 2125, 2370, 1010...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>2</td>\n",
       "      <td>eezlygj</td>\n",
       "      <td>[101, 2339, 1996, 6616, 2003, 3016, 3238, 1116...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  emotion       id  \\\n",
       "0  Now if he does off himself, everyone will thin...       27  ed00q6i   \n",
       "1                     WHY THE FUCK IS BAYLESS ISOING        2  eezlygj   \n",
       "\n",
       "                                  tokenized_comments  \n",
       "0  [101, 2085, 2065, 2002, 2515, 2125, 2370, 1010...  \n",
       "1  [101, 2339, 1996, 6616, 2003, 3016, 3238, 1116...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the comments\n",
    "train_data['tokenized_comments'] = train_data['comment'].apply(\n",
    "    lambda x: tokenizer.encode(x)\n",
    ")\n",
    "\n",
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's another way to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the tokenizer function\n",
    "\n",
    "#### PADDING means that we add padding to the tokens\n",
    "# for example\n",
    "# if we have the tokens [1, 2, 3, 4, 5]\n",
    "# and we want to pad them to the length of 10\n",
    "# we will add 5 padding tokens to the tokens\n",
    "# so the tokens will look like this: [1, 2, 3, 4, 5, 0, 0, 0, 0, 0]\n",
    "\n",
    "#### TRUNCATION means that we remove tokens from the tokens\n",
    "# for example\n",
    "# if we have the tokens [1, 2, 3, 4, 5]\n",
    "# and we want to truncate them to the length of 3\n",
    "# we will remove the last 2 tokens\n",
    "# so the tokens will look like this: [1, 2, 3]\n",
    "\n",
    "# Why do we need PADDING and TRUNCATION?\n",
    "# The BERT model requires that all the input sequences have the same length\n",
    "# we can achieve this by either padding or truncating the sequences\n",
    "# we can also use a combination of both\n",
    "# for example, using both\n",
    "# we can pad the sequences to a certain length\n",
    "# and if the sequence is longer than the maximum length\n",
    "# we can truncate the sequence to the maximum length\n",
    "\n",
    "#### RETURN_TENSORS means that we want the output to be a PyTorch tensor\n",
    "\n",
    "tokenized_comments = tokenizer(train_data['comment'].to_list(), padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2085, 2065,  ...,    0,    0,    0],\n",
       "        [ 101, 2339, 1996,  ...,    0,    0,    0],\n",
       "        [ 101, 2000, 2191,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2054, 2024,  ...,    0,    0,    0],\n",
       "        [ 101, 2062, 2066,  ...,    0,    0,    0],\n",
       "        [ 101, 5959, 1996,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### TOKENIZED_COMMENTS is a dictionary\n",
    "# it contains the INPUT_IDS, ATTENTION_MASK, and TOKEN_TYPE_IDS\n",
    "# input_ids are the tokenized comments\n",
    "\n",
    "#### ATTENTION_MASK is a tensor that has the same length as the input_ids\n",
    "# it contains 1s where the input_ids are and 0s where the padding tokens are\n",
    "\n",
    "#### TOKEN_TYPE_IDS is a tensor that has the same length as the input_ids\n",
    "# it contains 0s where the first sentence is and 1s where the second sentence is\n",
    "# since we only have one sentence, all the values are 0s\n",
    "\n",
    "# So it separates the text into sentences?\n",
    "# Yes, it separates the text into sentences\n",
    "# but since we only have one sentence, all the values are 0s\n",
    "\n",
    "tokenized_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  [[ 101 2085 2065 ...    0    0    0]\n",
      " [ 101 2339 1996 ...    0    0    0]\n",
      " [ 101 2000 2191 ...    0    0    0]\n",
      " ...\n",
      " [ 101 2054 2024 ...    0    0    0]\n",
      " [ 101 2062 2066 ...    0    0    0]\n",
      " [ 101 5959 1996 ...    0    0    0]]\n",
      "y =  [27  2 14 ...  3 13 17]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# These will be the features\n",
    "X_train = tokenized_comments['input_ids'].numpy()\n",
    "# And these will be the targets\n",
    "y_train = train_data['emotion'].values\n",
    "\n",
    "###########\n",
    "# We don't split the data into training and testing sets since they are already split\n",
    "\n",
    "# # Define stratified shuffle split\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# strat_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# for train_index, test_index in strat_split.split(X, y):\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "# print(\"Training set class distribution:\")\n",
    "# print(np.bincount(y_train))\n",
    "# print(\"Testing set class distribution:\")\n",
    "# print(np.bincount(y_test))\n",
    "###########\n",
    "\n",
    "# So this will be the dataset that we will use\n",
    "print(\"X = \", X_train)\n",
    "print(\"y = \", y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
